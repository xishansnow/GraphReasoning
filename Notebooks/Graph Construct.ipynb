{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12eb3ec3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b58d784",
   "metadata": {},
   "outputs": [],
   "source": [
    "from GraphReasoning.graph_generation import *\n",
    "from GraphReasoning.graph_analysis import *\n",
    "from GraphReasoning.graph_tools import *\n",
    "from GraphReasoning.llm_providers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfead03e",
   "metadata": {},
   "source": [
    "输入文件，可以是 pdf、markdown、txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ee78ba",
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Prompt user to input a file\n",
    "; file_path = input(\"请输入文件路径 (PDF、Markdown 或 TXT 文件): \").strip()\n",
    "try:\n",
    "    import tkinter as tk\n",
    "    from tkinter import filedialog\n",
    "\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    selected_path = filedialog.askopenfilename(\n",
    "        title=\"选择文件\",\n",
    "        filetypes=[\n",
    "            (\"PDF\", \"*.pdf\"),\n",
    "            (\"Markdown\", \"*.md *.markdown\"),\n",
    "            (\"Text\", \"*.txt\"),\n",
    "            (\"All files\", \"*.*\"),\n",
    "        ],\n",
    "    )\n",
    "    if selected_path:\n",
    "        file_path = selected_path\n",
    "    root.destroy()\n",
    "except Exception as e:\n",
    "    print(f\"文件选择窗口打开失败，改用手动输入路径: {e}\")\n",
    "\n",
    "# Validate the file exists and has correct extension\n",
    "valid_extensions = {'.pdf', '.md', '.markdown', '.txt'}\n",
    "file_extension = Path(file_path).suffix.lower()\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    print(\"错误: 文件不存在，请检查路径\")\n",
    "elif file_extension not in valid_extensions:\n",
    "    print(f\"错误: 不支持的文件格式。请使用以下格式之一: {', '.join(valid_extensions)}\")\n",
    "else:\n",
    "    print(f\"✓ 文件已加载: {file_path}\")\n",
    "    source_file = file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cdbd1b",
   "metadata": {},
   "source": [
    "调用 graph_generation.py 中的函数，从输入文件中提取文本，并生成 chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217356a6",
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "outputs": [],
   "source": [
    "# 从文件提取文本并生成 chunks（根据 graph_generation.py 中的函数命名调整）\n",
    "text = extract_text_from_file(source_file)\n",
    "chunks = generate_chunks(text, chunk_size=500, overlap=50)\n",
    "\n",
    "print(f\"✓ 已生成 {len(chunks)} 个 chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeebdb4",
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 如果本仓库不在 sys.path，可先添加\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))  # Notebook 位于 Notebooks/ 下，上一层是项目根\n",
    "\n",
    "from GraphReasoning import graph_generation\n",
    "\n",
    "# 选择/构造一个 generate 函数\n",
    "# 1) 使用已有的 provider 工厂（推荐）：见 graph_generation.py 中 main 的用法\n",
    "from GraphReasoning.llm_providers import get_generate_fn\n",
    "import os\n",
    "\n",
    "provider = \"openai\"  # 可换为 deepseek / qwen / llama_cpp / transformers\n",
    "provider_config = {\n",
    "    \"api_key\": os.getenv(\"OPENAI_API_KEY\", \"\"),\n",
    "    \"model\": os.getenv(\"OPENAI_MODEL\", \"gpt-4-turbo\"),\n",
    "    # deepseek/qwen 需额外 base_url；llama_cpp 需 model_path；transformers 需 model 名称\n",
    "}\n",
    "generate = get_generate_fn(provider, provider_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0695023",
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 方案 1: 对所有 chunks 进行知识图谱提取，然后合并结果\n",
    "knowledge_graphs = []\n",
    "for i, chunk in enumerate(chunks, start=1):\n",
    "    # 调用核心函数：从文本生成图\n",
    "    graph_html, graph_graphml, G, net, output_pdf = graph_generation.make_graph_from_text(\n",
    "        txt=chunk,\n",
    "        generate=generate,\n",
    "        include_contextual_proximity=True,\n",
    "        graph_root=\"notebook_test\",\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=0,\n",
    "        repeat_refine=0,\n",
    "        verbatim=True,\n",
    "        data_dir=\"./test_output/\",\n",
    "        save_PDF=False,\n",
    "        save_HTML=True,\n",
    "    )\n",
    "    knowledge_graphs.append((graph_html, graph_graphml, G, net, output_pdf))\n",
    "    print(f\"✓ 已处理 chunk {i}/{len(chunks)}\")\n",
    "\n",
    "print(\"HTML:\", graph_html)\n",
    "print(\"GraphML:\", graph_graphml)\n",
    "print(\"Nodes:\", G.number_of_nodes())\n",
    "print(\"Edges:\", G.number_of_edges())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c21f100",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m graph_graphml_agg = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     23\u001b[39m G_agg = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mchunks\u001b[49m, start=\u001b[32m1\u001b[39m):\n\u001b[32m     26\u001b[39m     t0 = time.time()\n\u001b[32m     27\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== 增量处理 chunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ===\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'chunks' is not defined"
     ]
    }
   ],
   "source": [
    "# 方案2：逐块增量合并（add_new_subgraph_from_text）\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from GraphReasoning.graph_tools import update_node_embeddings\n",
    "import networkx as nx\n",
    "import shutil\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# 准备轻量嵌入模型（可按需换更大模型）\n",
    "embed_model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(embed_model_name)\n",
    "model = AutoModel.from_pretrained(embed_model_name)\n",
    "\n",
    "# 初始空嵌入（如已有可加载）\n",
    "node_embeddings = {}\n",
    "\n",
    "# 临时目录\n",
    "data_dir_output = \"./test_output/\"\n",
    "Path(data_dir_output).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "graph_graphml_agg = None\n",
    "G_agg = None\n",
    "\n",
    "for i, chunk in enumerate(chunks, start=1):\n",
    "    t0 = time.time()\n",
    "    print(f\"\\n=== 增量处理 chunk {i}/{len(chunks)} ===\")\n",
    "    \n",
    "    # 第一次：直接生成基图\n",
    "    if G_agg is None:\n",
    "        graph_html, graph_graphml, G_agg, net, output_pdf = graph_generation.make_graph_from_text(\n",
    "            txt=chunk,\n",
    "            generate=generate,\n",
    "            include_contextual_proximity=True,\n",
    "            graph_root=f\"notebook_incremental_base_{i}\",\n",
    "            chunk_size=500,\n",
    "            chunk_overlap=0,\n",
    "            repeat_refine=0,\n",
    "            verbatim=True,\n",
    "            data_dir=data_dir_output,\n",
    "            save_PDF=False,\n",
    "            save_HTML=True,\n",
    "        )\n",
    "        graph_graphml_agg = graph_graphml\n",
    "        print(f\"基图完成: {graph_graphml_agg}\")\n",
    "    else:\n",
    "        # 后续：增量合并新子图\n",
    "        graph_GraphML_new, G_new, G_loaded, G_orig, node_embeddings, res = graph_generation.add_new_subgraph_from_text(\n",
    "            txt=chunk,\n",
    "            generate=generate,\n",
    "            node_embeddings=node_embeddings,\n",
    "            tokenizer=tokenizer,\n",
    "            model=model,\n",
    "            original_graph_path_and_fname=graph_graphml_agg,\n",
    "            data_dir_output=data_dir_output,\n",
    "            verbatim=True,\n",
    "            size_threshold=0,\n",
    "            chunk_size=500,\n",
    "            do_Louvain_on_new_graph=False,\n",
    "            include_contextual_proximity=False,\n",
    "            repeat_refine=0,\n",
    "            similarity_threshold=0.9,\n",
    "            do_simplify_graph=False,\n",
    "            return_only_giant_component=False,\n",
    "            save_common_graph=False,\n",
    "        )\n",
    "        # 更新聚合结果\n",
    "        G_agg = G_new\n",
    "        graph_graphml_agg = graph_GraphML_new\n",
    "        print(f\"增量图完成: {graph_graphml_agg}\")\n",
    "    \n",
    "    print(f\"当前节点/边: {G_agg.number_of_nodes()} / {G_agg.number_of_edges()}\")\n",
    "    print(f\"耗时: {time.time() - t0:.1f}s\")\n",
    "\n",
    "print(\"\\n最终聚合图:\", graph_graphml_agg)\n",
    "print(\"最终节点/边:\", G_agg.number_of_nodes(), G_agg.number_of_edges())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphreasoning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
